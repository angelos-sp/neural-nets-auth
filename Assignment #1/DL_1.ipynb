{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"DL_1.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPgiIve9iQF46xRXObupGza"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"rgWFCKoNZoAv"},"source":["# **MULTI-LAYER PERCEPTRON IMPLEMENTATION FOR CIFAR-10**\n","\n","*Spyrakis Angelos, ECE AUTh (9352)*\n","\n","\n","---\n","Table of Contents:\n","\n","**1. Fully-connected MLP**\n","\n","> 1.1. Fine tuning with KerasTuner\n",">\n","> 1.2. Fine tuning with KerasTuner (+ Dropout)\n",">\n","> 1.3. Fine-tuned model\n","\n","\n","**2. Convolutional Neural Network**\n","\n","> 2.1.   CNN Implementation (RGB)\n",">\n","> 2.2. CNN Implementation (Grayscale)\n","\n","---\n","Execute the two following cells to download the dataset and the tuner used."]},{"cell_type":"code","metadata":{"id":"eDGN6Zb0gqKZ"},"source":["!wget -c https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz -O - | tar -xz"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Wm3BZgNZh94"},"source":["pip install -q -U keras-tuner"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dXcgGL8xEQvn"},"source":["# 1. Fully-connected MLP\n"]},{"cell_type":"markdown","metadata":{"id":"6m7FJiSlatUe"},"source":["## 1.1. Fine-tuning using a keras tuner"]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"1xO53A8Ug2VB"},"source":["\"\"\"\n","Solving the CIFAR-10 classification task with a\n","Multi-Layer Perceptron NN structure.\n","\n","Fine-tuning using the Random Search keras tuner.\n","\"\"\"\n","import matplotlib.pyplot as plt\n","from keras import backend as K\n","import keras_tuner as kt\n","import tensorflow as tf\n","import numpy as np\n","import time\n","from sklearn.preprocessing import LabelBinarizer\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras.models import Sequential\n","\n","\n","def unpickle(file):\n","    \"\"\"\n","    Unpickles files produced with cPickle\n","    \"\"\"\n","    import pickle\n","    with open(file, 'rb') as fo:\n","        dict = pickle.load(fo, encoding='bytes')\n","    return dict\n","\n","\n","def load_cifar10():\n","    \"\"\"\n","    Loads the CIFAR-10 dataset\n","\n","    Output:\n","    X_train -> training images (RGB)\n","    y_train -> training images' labels\n","    X_test -> testing images (RGB)\n","    y_test -> testing images' labels\n","    \"\"\"\n","    dict = unpickle(\"cifar-10-batches-py/data_batch_1\")\n","    X_train = dict[b'data']\n","    y_train = np.array(dict[b'labels'], dtype=int)\n","\n","    dict = unpickle(\"cifar-10-batches-py/data_batch_2\")\n","    X_train = np.concatenate((X_train, dict[b'data']), axis=0)\n","    y_train = np.concatenate((y_train, np.array(dict[b'labels'], dtype=int)), axis=0)\n","\n","    dict = unpickle(\"cifar-10-batches-py/data_batch_3\")\n","    X_train = np.concatenate((X_train, dict[b'data']), axis=0)\n","    y_train = np.concatenate((y_train, np.array(dict[b'labels'], dtype=int)), axis=0)\n","\n","    dict = unpickle(\"cifar-10-batches-py/data_batch_4\")\n","    X_train = np.concatenate((X_train, dict[b'data']), axis=0)\n","    y_train = np.concatenate((y_train, np.array(dict[b'labels'], dtype=int)), axis=0)\n","\n","    dict = unpickle(\"cifar-10-batches-py/data_batch_5\")\n","    X_train = np.concatenate((X_train, dict[b'data']), axis=0)\n","    y_train = np.concatenate((y_train, np.array(dict[b'labels'], dtype=int)), axis=0)\n","\n","    dict = unpickle(\"cifar-10-batches-py/test_batch\")\n","    X_test = dict[b'data']\n","    y_test = np.array(dict[b'labels'], dtype=int)\n","\n","    # Normalize data\n","    X_train, X_test = X_train.astype('float32') / 255.0, X_test.astype('float32') / 255.0\n","\n","    # One-hot encode output, because allowing the model to assume a natural ordering \n","    # between categories may result in poor performance or unexpected results\n","    lb = LabelBinarizer()\n","    y_train = lb.fit_transform(y_train)  #fit_transform learns the mean and variance of the data\n","    y_test_original = y_test  #maintain original labels\n","    y_test = lb.transform(y_test)  #tansform uses the same mean and var as above\n","\n","    return X_train, y_train, X_test, y_test\n","\n","\n","def recall_m(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    return recall\n","\n","\n","def precision_m(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    return precision\n","\n","\n","def f1_metric(y_true, y_pred):\n","    precision = precision_m(y_true, y_pred)\n","    recall = recall_m(y_true, y_pred)\n","    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n","\n","\n","def build_model(hp):\n","    num_of_classes = 10\n","    input_shape = (32*32*3,)\n","\n","    # Create the model\n","    model = Sequential()\n","    initializer = tf.keras.initializers.HeUniform()\n","\n","    hp_units_first = hp.Choice('units_first', values=[2048, 1024, 512, 256])\n","    model.add(Dense(hp_units_first, input_shape=input_shape, activation='relu', kernel_initializer=initializer))\n","\n","    hp_units_second = hp.Choice('units_second', values=[2048, 1024, 512, 256])\n","    model.add(Dense(hp_units_second, activation='relu', kernel_initializer=initializer))\n","\n","    model.add(Dropout(0.3))\n","\n","    model.add(Dense(num_of_classes, activation='softmax'))\n","\n","    # Configure the model and start training\n","    hp_lr = hp.Choice('learning_rate', values=[0.1, 0.01, 0.001])\n","    opt = tf.keras.optimizers.Adam(learning_rate = hp_lr)\n","\n","    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[tf.keras.metrics.CategoricalAccuracy(), f1_metric])\n","\n","    return model\n","\n","\n","def main():\n","    # Load and process data\n","    X_train, y_train, X_test, y_test = load_cifar10()\n","\n","    # Set up tuner\n","    tuner = kt.RandomSearch(build_model, objective=kt.Objective('val_f1_metric', direction='max'), max_trials= 48, executions_per_trial=4, overwrite=True)\n","\n","    start_time = time.time()\n","\n","    tuner.search(X_train, y_train, epochs=60, batch_size=1000, validation_split=0.2)\n","\n","    elapsed_time = time.time() - start_time\n","    print(f'Tuning time = {elapsed_time} second(s)')\n","\n","    best_hps=tuner.get_best_hyperparameters(1)[0]\n","    \n","    print(f\"\"\"\n","    The hyperparameter search is complete. The optimal number of units in the first densely-connected\n","    layer is {best_hps.get('units_first')}, in the second densely-connected layer is {best_hps.get('units_second')} and the optimal learning rate \n","    for the optimizer is {best_hps.get('learning_rate')}.\n","    \"\"\")\n","\n","\n","if __name__ == \"__main__\":\n","    main()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ETOmYqzvWK_Y"},"source":["## 1.2. Fine-tuning using a keras tuner (+ Dropout)"]},{"cell_type":"code","metadata":{"id":"ywZ03m93WTZw"},"source":["\"\"\"\n","Solving the CIFAR-10 classification task with a\n","Multi-Layer Perceptron NN structure.\n","\n","Fine-tuning using the Random Search keras tuner (with Dropout).\n","\"\"\"\n","import matplotlib.pyplot as plt\n","from keras import backend as K\n","import keras_tuner as kt\n","import tensorflow as tf\n","import numpy as np\n","import time\n","from sklearn.preprocessing import LabelBinarizer\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras.models import Sequential\n","\n","\n","def unpickle(file):\n","    \"\"\"\n","    Unpickles files produced with cPickle\n","    \"\"\"\n","    import pickle\n","    with open(file, 'rb') as fo:\n","        dict = pickle.load(fo, encoding='bytes')\n","    return dict\n","\n","\n","def load_cifar10():\n","    \"\"\"\n","    Loads the CIFAR-10 dataset\n","\n","    Output:\n","    X_train -> training images (RGB)\n","    y_train -> training images' labels\n","    X_test -> testing images (RGB)\n","    y_test -> testing images' labels\n","    \"\"\"\n","    dict = unpickle(\"cifar-10-batches-py/data_batch_1\")\n","    X_train = dict[b'data']\n","    y_train = np.array(dict[b'labels'], dtype=int)\n","\n","    dict = unpickle(\"cifar-10-batches-py/data_batch_2\")\n","    X_train = np.concatenate((X_train, dict[b'data']), axis=0)\n","    y_train = np.concatenate((y_train, np.array(dict[b'labels'], dtype=int)), axis=0)\n","\n","    dict = unpickle(\"cifar-10-batches-py/data_batch_3\")\n","    X_train = np.concatenate((X_train, dict[b'data']), axis=0)\n","    y_train = np.concatenate((y_train, np.array(dict[b'labels'], dtype=int)), axis=0)\n","\n","    dict = unpickle(\"cifar-10-batches-py/data_batch_4\")\n","    X_train = np.concatenate((X_train, dict[b'data']), axis=0)\n","    y_train = np.concatenate((y_train, np.array(dict[b'labels'], dtype=int)), axis=0)\n","\n","    dict = unpickle(\"cifar-10-batches-py/data_batch_5\")\n","    X_train = np.concatenate((X_train, dict[b'data']), axis=0)\n","    y_train = np.concatenate((y_train, np.array(dict[b'labels'], dtype=int)), axis=0)\n","\n","    dict = unpickle(\"cifar-10-batches-py/test_batch\")\n","    X_test = dict[b'data']\n","    y_test = np.array(dict[b'labels'], dtype=int)\n","\n","    # Normalize data\n","    X_train, X_test = X_train.astype('float32') / 255.0, X_test.astype('float32') / 255.0\n","\n","    # One-hot encode output, because allowing the model to assume a natural ordering \n","    # between categories may result in poor performance or unexpected results\n","    lb = LabelBinarizer()\n","    y_train = lb.fit_transform(y_train)  #fit_transform learns the mean and variance of the data\n","    y_test_original = y_test  #maintain original labels\n","    y_test = lb.transform(y_test)  #tansform uses the same mean and var as above\n","\n","    return X_train, y_train, X_test, y_test\n","\n","\n","def recall_m(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    return recall\n","\n","\n","def precision_m(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    return precision\n","\n","\n","def f1_metric(y_true, y_pred):\n","    precision = precision_m(y_true, y_pred)\n","    recall = recall_m(y_true, y_pred)\n","    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n","\n","\n","def build_model(hp):\n","    num_of_classes = 10\n","    input_shape = (32*32*3,)\n","\n","    # Create the model\n","    model = Sequential()\n","    initializer = tf.keras.initializers.HeUniform()\n","\n","    hp_units_first = hp.Choice('units_first', values=[2048, 1024, 512, 256])\n","    model.add(Dense(hp_units_first, input_shape=input_shape, activation='relu', kernel_initializer=initializer))\n","\n","    hp_units_second = hp.Choice('units_second', values=[2048, 1024, 512, 256])\n","    model.add(Dense(hp_units_second, activation='relu', kernel_initializer=initializer))\n","\n","    hp_dropout_prob = hp.Choice('dropout_prob', values=[0.25, 0.3, 0.4])\n","    model.add(Dropout(hp_dropout_prob))\n","\n","    model.add(Dense(num_of_classes, activation='softmax'))\n","\n","    # Configure the model and start training\n","    opt = tf.keras.optimizers.Adam(learning_rate = 0.001)\n","\n","    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[tf.keras.metrics.CategoricalAccuracy(), f1_metric])\n","\n","    return model\n","\n","\n","def main():\n","    # Load and process data\n","    X_train, y_train, X_test, y_test = load_cifar10()\n","\n","    # Set up tuner\n","    tuner = kt.RandomSearch(build_model, objective=kt.Objective('val_f1_metric', direction='max'), max_trials= 48, executions_per_trial=4, overwrite=True)\n","\n","    start_time = time.time()\n","\n","    tuner.search(X_train, y_train, epochs=60, batch_size=1000, validation_split=0.2)\n","\n","    elapsed_time = time.time() - start_time\n","    print(f'Tuning time = {elapsed_time} second(s)')\n","\n","    best_hps=tuner.get_best_hyperparameters(1)[0]\n","    \n","    print(f\"\"\"\n","    The hyperparameter search is complete. The optimal number of units in the first densely-connected\n","    layer is {best_hps.get('units_first')}, in the second densely-connected layer is {best_hps.get('units_second')} and the optimal dropout probability \n","    is {best_hps.get('dropout_prob')}.\n","    \"\"\")\n","\n","\n","if __name__ == \"__main__\":\n","    main()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"unmfju79aOgm"},"source":["## 1.3. Fine-tuned model"]},{"cell_type":"code","metadata":{"id":"cgcJ7b_cYyd0"},"source":["\"\"\"\n","Solving the CIFAR-10 classification task with a\n","Multi-Layer Perceptron NN structure.\n","\n","Fine-Tuned Network\n","\"\"\"\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from keras import backend as K\n","import numpy as np\n","import time\n","from sklearn.preprocessing import LabelBinarizer\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras.models import Sequential\n","from sklearn.metrics import ConfusionMatrixDisplay\n","from sklearn.metrics import confusion_matrix\n","\n","\n","def unpickle(file):\n","    \"\"\"\n","    Unpickles files produced with cPickle\n","    \"\"\"\n","    import pickle\n","    with open(file, 'rb') as fo:\n","        dict = pickle.load(fo, encoding='bytes')\n","    return dict\n","\n","\n","def load_cifar10():\n","    \"\"\"\n","    Loads the CIFAR-10 dataset\n","\n","    Output:\n","    X_train -> training images (RGB)\n","    y_train -> training images' labels\n","    X_test -> testing images (RGB)\n","    y_test -> testing images' labels\n","    \"\"\"\n","    dict = unpickle(\"cifar-10-batches-py/data_batch_1\")\n","    X_train = dict[b'data']\n","    y_train = np.array(dict[b'labels'], dtype=int)\n","\n","    dict = unpickle(\"cifar-10-batches-py/data_batch_2\")\n","    X_train = np.concatenate((X_train, dict[b'data']), axis=0)\n","    y_train = np.concatenate((y_train, np.array(dict[b'labels'], dtype=int)), axis=0)\n","\n","    dict = unpickle(\"cifar-10-batches-py/data_batch_3\")\n","    X_train = np.concatenate((X_train, dict[b'data']), axis=0)\n","    y_train = np.concatenate((y_train, np.array(dict[b'labels'], dtype=int)), axis=0)\n","\n","    dict = unpickle(\"cifar-10-batches-py/data_batch_4\")\n","    X_train = np.concatenate((X_train, dict[b'data']), axis=0)\n","    y_train = np.concatenate((y_train, np.array(dict[b'labels'], dtype=int)), axis=0)\n","\n","    dict = unpickle(\"cifar-10-batches-py/data_batch_5\")\n","    X_train = np.concatenate((X_train, dict[b'data']), axis=0)\n","    y_train = np.concatenate((y_train, np.array(dict[b'labels'], dtype=int)), axis=0)\n","\n","    dict = unpickle(\"cifar-10-batches-py/test_batch\")\n","    X_test = dict[b'data']\n","    y_test = np.array(dict[b'labels'], dtype=int)\n","\n","    # Normalize data\n","    X_train, X_test = X_train.astype('float32') / 255.0, X_test.astype('float32') / 255.0\n","\n","    # One-hot encode output, because allowing the model to assume a natural ordering \n","    # between categories may result in poor performance or unexpected results\n","    lb = LabelBinarizer()\n","    y_train = lb.fit_transform(y_train)  #fit_transform learns the mean and variance of the data\n","    y_test_original = y_test  #maintain original labels\n","    y_test = lb.transform(y_test)  #tansform uses the same mean and var as above\n","\n","    return X_train, y_train, X_test, y_test, y_test_original\n","\n","\n","def plot_model(history):\n","    # Plot accuracy\n","    plt.plot(history.history['categorical_accuracy'])\n","    plt.plot(history.history['val_categorical_accuracy'])\n","    plt.title(f'Model Accuracy')\n","    plt.ylabel('accuracy')\n","    plt.xlabel('epoch')\n","    plt.legend(['train', 'val'], loc='upper left')\n","    plt.show()\n","\n","    # Plot loss\n","    plt.plot(history.history['loss'])\n","    plt.plot(history.history['val_loss'])\n","    plt.title(f'Model Loss')\n","    plt.ylabel('loss')\n","    plt.xlabel('epoch')\n","    plt.legend(['train', 'val'], loc='upper left')\n","    plt.show()\n","\n","\n","def recall_m(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    return recall\n","\n","\n","def precision_m(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    return precision\n","\n","\n","def f1_metric(y_true, y_pred):\n","    precision = precision_m(y_true, y_pred)\n","    recall = recall_m(y_true, y_pred)\n","    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n","\n","\n","def build_model():\n","    num_of_classes = 10\n","    input_shape = (32*32*3,)\n","\n","    # Create the model\n","    model = Sequential()\n","    initializer = tf.keras.initializers.HeUniform()\n","\n","    model.add(Dense(2048, input_shape=input_shape, activation='relu', kernel_initializer=initializer))\n","    model.add(Dense(1024, activation='relu', kernel_initializer=initializer))\n","    model.add(Dropout(0.25))\n","    model.add(Dense(num_of_classes, activation='softmax'))\n","\n","    # Configure the model and start training\n","    opt = tf.keras.optimizers.Adam(learning_rate = 0.001)\n","    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[tf.keras.metrics.CategoricalAccuracy(), f1_metric])\n","\n","    return model\n","\n","\n","def print_examples(y_test, y_pred, X_test_flattened, labels):\n","    X_test = X_test_flattened.reshape(10000, 3, 32, 32).transpose(0,2,3,1)\n","\n","    print('Printing examples of valid classification:')\n","    printed = 0\n","    fig = plt.figure(figsize=(4,4))\n","    for i in range (0, y_test.shape[0]):\n","        if y_test[i] == y_pred[i]:\n","            ax = fig.add_subplot(2, 2, printed + 1, xticks=[], yticks=[])\n","            ax.title.set_text(labels[y_pred[i]])\n","            ax.imshow((X_test[i, :]*255).astype(int))\n","            printed += 1\n","        if printed == 4:\n","            printed = 0\n","            break\n","    plt.show()\n","    print('Printing examples of false classification:')\n","    fig2 = plt.figure(figsize=(4,4))\n","    for i in range (0, y_test.shape[0]):\n","        if y_test[i] !=  y_pred[i]:\n","            ax2 = fig2.add_subplot(2, 2, printed + 1, xticks=[], yticks=[])\n","            ax2.title.set_text(labels[y_pred[i]])\n","            ax2.imshow((X_test[i, :]*255).astype(int))\n","            printed += 1\n","        if printed == 4:\n","            break\n","    plt.show()\n","\n","\n","def main():\n","    X_train, y_train, X_test, y_test, y_test_original = load_cifar10()\n","    model = build_model()\n","    start_time = time.time()\n","\n","    history = model.fit(X_train, y_train, epochs=60, batch_size=1000, verbose=1, validation_split=0.2)\n","\n","    elapsed_time = time.time() - start_time\n","    print(f'Training time = {elapsed_time} second(s)')\n","\n","    plot_model(history)\n","\n","    # Test the model after training\n","    start_time = time.time()\n","\n","    test_results = model.evaluate(X_test, y_test, verbose=1)\n","    print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]*100}%')\n","    \n","    elapsed_time = time.time() - start_time\n","    print(f'Testing time = {elapsed_time} second(s)')\n","\n","    # Plot confusion matrix\n","    prediction = model.predict(X_test) \n","    y_pred = np.argmax(prediction, axis=1) #get predicted class\n","    labels = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n","    cm = confusion_matrix(y_test_original, y_pred)\n","    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n","    fig, ax = plt.subplots(figsize=(15, 15))\n","    disp.plot(cmap=plt.cm.Blues, ax=ax)\n","    plt.show()\n","\n","    print_examples(y_test_original, y_pred, X_test, labels)\n","\n","\n","if __name__ == \"__main__\":\n","    main()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-w2cHQebEnI9"},"source":["# 2. Convolutional Neural Network"]},{"cell_type":"markdown","metadata":{"id":"nogwbrvZyaKn"},"source":["## 2.1 CNN Implementation (RGB)"]},{"cell_type":"code","metadata":{"id":"oKjLLJvqmjAg"},"source":["\"\"\"\n","Solving the CIFAR-10 classification task with a\n","Multi-Layer Perceptron NN structure.\n","\n","Fine-Tuned Network\n","\"\"\"\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from keras import backend as K\n","import numpy as np\n","import time\n","from sklearn.preprocessing import LabelBinarizer\n","from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n","from tensorflow.keras.models import Sequential\n","from sklearn.metrics import ConfusionMatrixDisplay\n","from sklearn.metrics import confusion_matrix\n","\n","\n","def unpickle(file):\n","    \"\"\"\n","    Unpickles files produced with cPickle\n","    \"\"\"\n","    import pickle\n","    with open(file, 'rb') as fo:\n","        dict = pickle.load(fo, encoding='bytes')\n","    return dict\n","\n","\n","def load_cifar10():\n","    \"\"\"\n","    Loads the CIFAR-10 dataset\n","\n","    Output:\n","    X_train -> training images (RGB)\n","    y_train -> training images' labels\n","    X_test -> testing images (RGB)\n","    y_test -> testing images' labels\n","    \"\"\"\n","    dict = unpickle(\"cifar-10-batches-py/data_batch_1\")\n","    X_train = dict[b'data']\n","    y_train = np.array(dict[b'labels'], dtype=int)\n","\n","    dict = unpickle(\"cifar-10-batches-py/data_batch_2\")\n","    X_train = np.concatenate((X_train, dict[b'data']), axis=0)\n","    y_train = np.concatenate((y_train, np.array(dict[b'labels'], dtype=int)), axis=0)\n","\n","    dict = unpickle(\"cifar-10-batches-py/data_batch_3\")\n","    X_train = np.concatenate((X_train, dict[b'data']), axis=0)\n","    y_train = np.concatenate((y_train, np.array(dict[b'labels'], dtype=int)), axis=0)\n","\n","    dict = unpickle(\"cifar-10-batches-py/data_batch_4\")\n","    X_train = np.concatenate((X_train, dict[b'data']), axis=0)\n","    y_train = np.concatenate((y_train, np.array(dict[b'labels'], dtype=int)), axis=0)\n","\n","    dict = unpickle(\"cifar-10-batches-py/data_batch_5\")\n","    X_train = np.concatenate((X_train, dict[b'data']), axis=0)\n","    y_train = np.concatenate((y_train, np.array(dict[b'labels'], dtype=int)), axis=0)\n","\n","    dict = unpickle(\"cifar-10-batches-py/test_batch\")\n","    X_test = dict[b'data']\n","    y_test = np.array(dict[b'labels'], dtype=int)\n","\n","    # Reshape to (32, 32, 3)\n","    X_train = X_train.reshape(50000, 3, 32, 32).transpose(0,2,3,1).astype(int)\n","    X_test = X_test.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(int)\n","\n","    # Normalize data\n","    X_train, X_test = X_train.astype('float32') / 255.0, X_test.astype('float32') / 255.0\n","\n","    # One-hot encode output, because allowing the model to assume a natural ordering \n","    # between categories may result in poor performance or unexpected results\n","    lb = LabelBinarizer()\n","    y_train = lb.fit_transform(y_train)  #fit_transform learns the mean and variance of the data\n","    y_test_original = y_test  #maintain original labels\n","    y_test = lb.transform(y_test)  #tansform uses the same mean and var as above\n","\n","    return X_train, y_train, X_test, y_test, y_test_original\n","\n","\n","def plot_model(history):\n","    # Plot accuracy\n","    plt.plot(history.history['categorical_accuracy'])\n","    plt.plot(history.history['val_categorical_accuracy'])\n","    plt.title(f'Model Accuracy')\n","    plt.ylabel('accuracy')\n","    plt.xlabel('epoch')\n","    plt.legend(['train', 'val'], loc='upper left')\n","    plt.show()\n","\n","    # Plot loss\n","    plt.plot(history.history['loss'])\n","    plt.plot(history.history['val_loss'])\n","    plt.title(f'Model Loss')\n","    plt.ylabel('loss')\n","    plt.xlabel('epoch')\n","    plt.legend(['train', 'val'], loc='upper left')\n","    plt.show()\n","\n","\n","def recall_m(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    return recall\n","\n","\n","def precision_m(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    return precision\n","\n","\n","def f1_metric(y_true, y_pred):\n","    precision = precision_m(y_true, y_pred)\n","    recall = recall_m(y_true, y_pred)\n","    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n","\n","\n","def print_examples(y_test, y_pred, X_test, labels):\n","    print('Printing examples of valid classification:')\n","    printed = 0\n","    fig = plt.figure(figsize=(4,4))\n","    for i in range (0, y_test.shape[0]):\n","        if y_test[i] == y_pred[i]:\n","            ax = fig.add_subplot(2, 2, printed + 1, xticks=[], yticks=[])\n","            ax.title.set_text(labels[y_pred[i]])\n","            ax.imshow((X_test[i, :]*255).astype(int))\n","            printed += 1\n","        if printed == 4:\n","            printed = 0\n","            break\n","    plt.show()\n","    print('Printing examples of false classification:')\n","    fig2 = plt.figure(figsize=(4,4))\n","    for i in range (0, y_test.shape[0]):\n","        if y_test[i] !=  y_pred[i]:\n","            ax2 = fig2.add_subplot(2, 2, printed + 1, xticks=[], yticks=[])\n","            ax2.title.set_text(labels[y_pred[i]])\n","            ax2.imshow((X_test[i, :]*255).astype(int))\n","            printed += 1\n","        if printed == 4:\n","            break\n","    plt.show()\n","\n","\n","def build_model():\n","    num_of_classes = 10\n","    input_shape = (32, 32, 3)\n","\n","    # Create the model\n","    model = Sequential()\n","    initializer = tf.keras.initializers.HeUniform()\n","\n","    model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', \n","                            kernel_initializer=initializer, input_shape=input_shape))\n","    model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', \n","                            kernel_initializer=initializer))\n","    model.add(MaxPooling2D(2,2))\n","\n","    model.add(Dropout(0.2))\n","\n","    model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu', \n","                            kernel_initializer=initializer))\n","    model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu', \n","                            kernel_initializer=initializer))\n","    model.add(MaxPooling2D(2,2))\n","\n","    model.add(Dropout(0.2))\n","\n","    model.add(Conv2D(filters=128, kernel_size=3, padding='same', activation='relu', \n","                            kernel_initializer=initializer))\n","    model.add(Conv2D(filters=128, kernel_size=3, padding='same', activation='relu', \n","                            kernel_initializer=initializer))\n","    model.add(MaxPooling2D(2,2))\n","\n","    model.add(Dropout(0.2))\n","    \n","    model.add(Flatten())\n","    model.add(Dense(128, activation='relu', kernel_initializer=initializer))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(num_of_classes, activation='softmax'))\n","\n","    # Configure the model and start training\n","    opt = tf.keras.optimizers.Adam(learning_rate = 0.001)\n","    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[tf.keras.metrics.CategoricalAccuracy(), f1_metric])\n","\n","    return model\n","\n","\n","def main():\n","    X_train, y_train, X_test, y_test, y_test_original = load_cifar10()\n","    model = build_model()\n","    start_time = time.time()\n","\n","    history = model.fit(X_train, y_train, epochs=40, batch_size=1000, verbose=1, validation_split=0.2)\n","\n","    elapsed_time = time.time() - start_time\n","    print(f'Training time = {elapsed_time} second(s)')\n","\n","    plot_model(history)\n","\n","    # Test the model after training\n","    start_time = time.time()\n","\n","    test_results = model.evaluate(X_test, y_test, verbose=1)\n","    print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]*100}%')\n","    \n","    elapsed_time = time.time() - start_time\n","    print(f'Testing time = {elapsed_time} second(s)')\n","    \n","    # Plot confusion matrix\n","    prediction = model.predict(X_test) \n","    y_pred = np.argmax(prediction, axis=1) #get predicted class\n","    labels = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n","    cm = confusion_matrix(y_test_original, y_pred)\n","    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n","    fig, ax = plt.subplots(figsize=(15, 15))\n","    disp.plot(cmap=plt.cm.Blues, ax=ax)\n","    plt.show()\n","\n","    print_examples(y_test_original, y_pred, X_test, labels)\n","    \n","\n","if __name__ == \"__main__\":\n","    main()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KCdnmYGnwoVr"},"source":["## 2.2 CNN Implementation (Grayscale)"]},{"cell_type":"code","metadata":{"id":"nNqrVYZswuja"},"source":["\"\"\"\n","Solving the CIFAR-10 classification task with a\n","Multi-Layer Perceptron NN structure.\n","\n","Fine-Tuned Network\n","\"\"\"\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from keras import backend as K\n","import numpy as np\n","import time\n","import cv2\n","from sklearn.preprocessing import LabelBinarizer\n","from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n","from tensorflow.keras.models import Sequential\n","from sklearn.metrics import ConfusionMatrixDisplay\n","from sklearn.metrics import confusion_matrix\n","\n","\n","def unpickle(file):\n","    \"\"\"\n","    Unpickles files produced with cPickle\n","    \"\"\"\n","    import pickle\n","    with open(file, 'rb') as fo:\n","        dict = pickle.load(fo, encoding='bytes')\n","    return dict\n","\n","\n","def load_cifar10():\n","    \"\"\"\n","    Loads the CIFAR-10 dataset\n","\n","    Output:\n","    X_train -> training images (RGB)\n","    y_train -> training images' labels\n","    X_test -> testing images (RGB)\n","    y_test -> testing images' labels\n","    \"\"\"\n","    dict = unpickle(\"cifar-10-batches-py/data_batch_1\")\n","    X_train = dict[b'data']\n","    y_train = np.array(dict[b'labels'], dtype=int)\n","\n","    dict = unpickle(\"cifar-10-batches-py/data_batch_2\")\n","    X_train = np.concatenate((X_train, dict[b'data']), axis=0)\n","    y_train = np.concatenate((y_train, np.array(dict[b'labels'], dtype=int)), axis=0)\n","\n","    dict = unpickle(\"cifar-10-batches-py/data_batch_3\")\n","    X_train = np.concatenate((X_train, dict[b'data']), axis=0)\n","    y_train = np.concatenate((y_train, np.array(dict[b'labels'], dtype=int)), axis=0)\n","\n","    dict = unpickle(\"cifar-10-batches-py/data_batch_4\")\n","    X_train = np.concatenate((X_train, dict[b'data']), axis=0)\n","    y_train = np.concatenate((y_train, np.array(dict[b'labels'], dtype=int)), axis=0)\n","\n","    dict = unpickle(\"cifar-10-batches-py/data_batch_5\")\n","    X_train = np.concatenate((X_train, dict[b'data']), axis=0)\n","    y_train = np.concatenate((y_train, np.array(dict[b'labels'], dtype=int)), axis=0)\n","\n","    dict = unpickle(\"cifar-10-batches-py/test_batch\")\n","    X_test = dict[b'data']\n","    y_test = np.array(dict[b'labels'], dtype=int)\n","\n","    # Reshape to (32, 32, 3)\n","    X_train = X_train.reshape(50000, 3, 32, 32).transpose(0,2,3,1).astype(int)\n","    X_test = X_test.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(int)\n","\n","    # Convert to grayscale\n","    X_train = np.array([cv2.cvtColor(image.astype(np.uint8), cv2.COLOR_BGR2GRAY) for image in X_train])\n","    X_test = np.array([cv2.cvtColor(image.astype(np.uint8), cv2.COLOR_BGR2GRAY) for image in X_test])\n","\n","    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n","    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n","\n","    # Normalize data\n","    X_train, X_test = X_train.astype('float32') / 255.0, X_test.astype('float32') / 255.0\n","\n","    # One-hot encode output, because allowing the model to assume a natural ordering \n","    # between categories may result in poor performance or unexpected results\n","    lb = LabelBinarizer()\n","    y_train = lb.fit_transform(y_train)  #fit_transform learns the mean and variance of the data\n","    y_test_original = y_test  #maintain original labels\n","    y_test = lb.transform(y_test)  #tansform uses the same mean and var as above\n","\n","    return X_train, y_train, X_test, y_test, y_test_original\n","\n","\n","def plot_model(history):\n","    # Plot accuracy\n","    plt.plot(history.history['categorical_accuracy'])\n","    plt.plot(history.history['val_categorical_accuracy'])\n","    plt.title(f'Model Accuracy')\n","    plt.ylabel('accuracy')\n","    plt.xlabel('epoch')\n","    plt.legend(['train', 'val'], loc='upper left')\n","    plt.show()\n","\n","    # Plot loss\n","    plt.plot(history.history['loss'])\n","    plt.plot(history.history['val_loss'])\n","    plt.title(f'Model Loss')\n","    plt.ylabel('loss')\n","    plt.xlabel('epoch')\n","    plt.legend(['train', 'val'], loc='upper left')\n","    plt.show()\n","\n","\n","def recall_m(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    return recall\n","\n","\n","def precision_m(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    return precision\n","\n","\n","def f1_metric(y_true, y_pred):\n","    precision = precision_m(y_true, y_pred)\n","    recall = recall_m(y_true, y_pred)\n","    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n","\n","\n","def build_model():\n","    num_of_classes = 10\n","    input_shape = (32, 32, 1)\n","\n","    # Create the model\n","    model = Sequential()\n","    initializer = tf.keras.initializers.HeUniform()\n","\n","    model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', \n","                            kernel_initializer=initializer, input_shape=input_shape))\n","    model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', \n","                            kernel_initializer=initializer))\n","    model.add(MaxPooling2D(2,2))\n","\n","    model.add(Dropout(0.2))\n","\n","    model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu', \n","                            kernel_initializer=initializer))\n","    model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu', \n","                            kernel_initializer=initializer))\n","    model.add(MaxPooling2D(2,2))\n","\n","    model.add(Dropout(0.2))\n","\n","    model.add(Conv2D(filters=128, kernel_size=3, padding='same', activation='relu', \n","                            kernel_initializer=initializer))\n","    model.add(Conv2D(filters=128, kernel_size=3, padding='same', activation='relu', \n","                            kernel_initializer=initializer))\n","    model.add(MaxPooling2D(2,2))\n","\n","    model.add(Dropout(0.2))\n","    \n","    model.add(Flatten())\n","    model.add(Dense(128, activation='relu', kernel_initializer=initializer))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(num_of_classes, activation='softmax'))\n","\n","    # Configure the model and start training\n","    opt = tf.keras.optimizers.Adam(learning_rate = 0.001)\n","    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[tf.keras.metrics.CategoricalAccuracy(), f1_metric])\n","\n","    return model\n","\n","\n","def main():\n","    X_train, y_train, X_test, y_test, y_test_original = load_cifar10()\n","    model = build_model()\n","    start_time = time.time()\n","\n","    history = model.fit(X_train, y_train, epochs=40, batch_size=1000, verbose=1, validation_split=0.2)\n","\n","    elapsed_time = time.time() - start_time\n","    print(f'Training time = {elapsed_time} second(s)')\n","\n","    plot_model(history)\n","\n","    # Test the model after training\n","    start_time = time.time()\n","\n","    test_results = model.evaluate(X_test, y_test, verbose=1)\n","    print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]*100}%')\n","    \n","    elapsed_time = time.time() - start_time\n","    print(f'Testing time = {elapsed_time} second(s)')\n","    \"\"\"\n","    # Plot confusion matrix\n","    prediction = model.predict(X_test) \n","    y_pred = np.argmax(prediction, axis=1) #get predicted class\n","    labels = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n","    cm = confusion_matrix(y_test_original, y_pred)\n","    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n","    fig, ax = plt.subplots(figsize=(15, 15))\n","    disp.plot(cmap=plt.cm.Blues, ax=ax)\n","    plt.show()\n","    \"\"\"\n","\n","if __name__ == \"__main__\":\n","    main()"],"execution_count":null,"outputs":[]}]}